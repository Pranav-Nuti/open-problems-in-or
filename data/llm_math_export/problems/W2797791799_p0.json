{
  "problem_id": "W2797791799_p0",
  "paper_id": "W2797791799",
  "source_paper": "W2797791799",
  "source_paper_title": "A Variable Sample-Size Stochastic Quasi-Newton Method for Smooth and Nonsmooth Stochastic Convex Optimization",
  "source_paper_url": "https://doi.org/10.1287/moor.2021.1147",
  "problem_index": 0,
  "title": "Stochastic line-search analysis for variable sample-size stochastic quasi-Newton methods",
  "latex": "\\begin{problem}[Stochastic line-search VS-SQN]\\label{prob:stoch-linesearch-vs-sqn}\nLet \\((\\Omega,\\mathcal F,\\mathbb P)\\) be a probability space and let \\(\\xi: \\Omega\\to\\mathbb R^o\\) be a random vector. Consider the expectation-valued convex optimization problem\n\\[\n\\min_{x\\in\\mathbb R^n} f(x) \\coloneqq \\mathbb E\\big[F(x,\\xi)\\big],\n\\]\nwhere for \\(\\mathbb P\\)-a.e. \\(\\xi\\), the sample-path function \\(F(\\cdot,\\xi)\\) is convex and continuously differentiable. Assume that \\(f\\) is differentiable and either\n\\begin{itemize}\n\\item (strongly convex case) \\(f\\) is \\(\\tau\\)-strongly convex and has \\(L\\)-Lipschitz gradient (with \\(\\tau,L>0\\) unknown), or\n\\item (merely convex case) \\(f\\) is convex and has \\(L\\)-Lipschitz gradient (with \\(L>0\\) unknown).\n\\end{itemize}\nAssume access to a stochastic first-order oracle that, given \\(x\\), returns i.i.d. samples \\(\\nabla_x F(x,\\xi_i)\\) with\n\\[\n\\mathbb E[\\nabla_x F(x,\\xi)] = \\nabla f(x),\\qquad\n\\mathbb E\\big[\\|\\nabla_x F(x,\\xi)-\\nabla f(x)\\|^2\\mid x\\big] < \\infty.\n\\]\nA variable sample-size stochastic quasi-Newton (VS-SQN) step has the form\n\\[\n\\begin{aligned}\n g_k &= \\frac{1}{N_k}\\sum_{j=1}^{N_k} \\nabla_x F(x_k,\\xi_{j,k}),\\\\\n d_k &= -H_k g_k,\\\\\n x_{k+1} &= x_k + \\alpha_k d_k,\n\\end{aligned}\n\\]\nwhere \\(N_k\\in\\mathbb N\\) is the batch size, \\(H_k\\in\\mathbb R^{n\\times n}\\) is a (random) symmetric positive definite quasi-Newton inverse-Hessian approximation (e.g., L-BFGS-type) measurable w.r.t. the past, and \\(\\alpha_k>0\\) is a \\emph{stochastic line-search} stepsize computed from noisy function/gradient information.\n\n\\emph{Open problem:} Design a stochastic line-search rule to choose \\(\\alpha_k\\) (e.g., Armijo/Wolfe-type conditions based on additional Monte Carlo samples of \\(F\\) and/or directional derivatives) together with sampling rules \\(\\{N_k\\}\\) and quasi-Newton updates \\(\\{H_k\\}\\) such that one can prove non-asymptotic convergence guarantees (rates and oracle complexity) for \\(\\{x_k\\}\\) \\emph{without requiring knowledge of} \\(L\\) and/or \\(\\tau\\), while explicitly handling the key difficulty that \\(\\alpha_k\\) is random and generally statistically dependent on \\(d_k\\) and the gradient noise.\n\nConcretely, establish conditions under which one can guarantee, for some target accuracy \\(\\varepsilon>0\\), either\n\\begin{align*}\n\\text{(linear/strongly convex)}\\quad &\\mathbb E[f(x_K)-f^*] \\le \\varepsilon\\ \\text{with } K=\\mathcal O(\\log(1/\\varepsilon)),\\\\\n\\text{(sublinear/convex)}\\quad &\\mathbb E[f(x_K)-f^*] \\le \\varepsilon\\ \\text{with } K=\\mathcal O(1/\\varepsilon)\\ \\text{(or better)},\n\\end{align*}\nand derive corresponding bounds on total oracle calls (numbers of sampled gradients and function values) made by the line-search VS-SQN scheme.\n\\end{problem}\n",
  "context": "Explicitly raised in Remark 1 (Section 3.1) where the authors note that one avenue to avoid dependence on unknown Lipschitz/strong convexity parameters is to use line search, but in expectation-valued problems the resulting random steplength depends on the (random) direction so standard analysis fails; they state that obtaining such results with refined analysis \u201cremains the focus of future work.\u201d It is reiterated in Remark 5, item 5 (\u201cStochastic line-search techniques \u2026 remains a goal of future work.\u201d).",
  "subject_classification": "Stochastic Optimization",
  "keywords": [
    "stochastic quasi-Newton",
    "stochastic line search",
    "variable sample size",
    "L-BFGS",
    "oracle complexity",
    "non-asymptotic convergence"
  ],
  "importance": 7,
  "importance_scale": 10,
  "difficulty": 8,
  "difficulty_scale": 10,
  "extraction_model": "gpt-5.2",
  "extraction_reasoning_effort": "high",
  "verification": {
    "is_open": true,
    "methods_used": [
      "web_search"
    ],
    "source": null,
    "model": "gpt-5.2",
    "total_cost_usd": 0.29482775
  },
  "literature_review": {
    "model": "gpt-5.2",
    "total_cost_usd": 0.4842145,
    "sources": [
      {
        "bibtex": "@article{BollapragadaMudigereNocedalShiTang2018ProgressiveBatchingLBFGS,\n  author       = {Bollapragada, Raghu and Mudigere, Dheevatsa and Nocedal, Jorge and Shi, Hao-Jun Michael and Tang, Ping Tak Peter},\n  title        = {A Progressive Batching {L}-{BFGS} Method for Machine Learning},\n  year         = {2018},\n  journal      = {arXiv preprint},\n  eprint       = {1802.05374},\n  archivePrefix= {arXiv},\n  primaryClass = {math.OC},\n  doi          = {10.48550/arXiv.1802.05374},\n  note         = {ICML 2018 version in Proc. PMLR 80:620--629}\n}",
        "theorem_or_result": "The paper proposes a progressive-batching stochastic L-BFGS method with a stochastic Armijo backtracking line-search and an \u201cinner product quasi-Newton\u201d (IPQN) test for choosing batch sizes.\n\n**Convergence results (exact displayed inequalities not accessible in the ar5iv HTML rendering; theorem statements refer to equations (23)\u2013(27)):**\n- *Theorem 3.1* (strongly convex case): under twice continuous differentiability and uniform bounds on the Hessian eigenvalues (i.e., strong convexity and smoothness), with sample sizes chosen by the (exact-variance) IPQN test and an additional \u201corthogonality\u201d condition, and with uniformly bounded eigenvalues of the inverse-Hessian approximations, the fixed-stepsize stochastic quasi-Newton iteration achieves **linear convergence in expected suboptimality**, i.e., an inequality of the form \\(\\mathbb E[f(x_k)-f^*]\\le \\rho^k\\,\\mathbb E[f(x_0)-f^*]\\) for some \\(\\rho\\in(0,1)\\).\n- *Theorem 3.2* (nonconvex case): under a Lipschitz gradient assumption, the method yields **expected first-order stationarity** with a **global sublinear rate** for the best (smallest) expected gradient norms among the first \\(K\\) iterates (a bound of the form \\(\\min_{0\\le k < K}\\mathbb E\\|\\nabla f(x_k)\\|^2 \\le O(1/K)\\), up to constants).\n\nThe stochastic line-search component is motivated by controlling the predicted decrease in the *true* objective using statistics from the mini-batch, but the main formal rate theorems in the paper are stated for a fixed stepsize.",
        "relevance_category": "PARTIAL_PROGRESS",
        "exact_match": null,
        "connection": "This is the closest work to the open problem\u2019s target algorithmic template: it combines (i) variable/progressive sample sizes, (ii) quasi-Newton curvature updates (L-BFGS), and (iii) a stochastic Armijo-type line-search mechanism. Theorems provide non-asymptotic linear/sublinear guarantees, but the stated convergence analysis in the main theorems is for a fixed stepsize and relies on strong assumptions (e.g., bounded eigenvalues of inverse-Hessian approximations and exact-variance tests), so it does not fully resolve the dependence issues created by a truly random line search. It nonetheless provides concrete sampling tests (IPQN) and a blueprint for integrating stochastic line-search with (progressive) sampling rules."
      },
      {
        "bibtex": "@article{PaquetteScheinberg2020StochasticLineSearch,\n  author       = {Paquette, Courtney and Scheinberg, Katya},\n  title        = {A Stochastic Line Search Method with Expected Complexity Analysis},\n  journal      = {SIAM Journal on Optimization},\n  year         = {2020},\n  volume       = {30},\n  number       = {1},\n  pages        = {349--376},\n  doi          = {10.1137/18M1216250},\n  eprint       = {1807.07994},\n  archivePrefix= {arXiv},\n  primaryClass = {math.OC}\n}",
        "theorem_or_result": "The paper analyzes a stochastic Armijo backtracking line-search where function values and gradients are computed to a dynamically adjusted accuracy that holds with a fixed probability (a \u201cprobabilistically sufficiently accurate\u201d oracle model).\n\nMain results (precise theorem numbering/inequalities not accessible here; paraphrased from the paper\u2019s stated contributions in the abstract and metadata):\n- For smooth nonconvex objectives, the method achieves the standard worst-case iteration complexity (in expectation) for reaching an \\(\\varepsilon\\)-first-order stationary point, matching deterministic first-order rates up to constants.\n- For convex and strongly convex smooth objectives, the method achieves the corresponding deterministic gradient-descent-type rates **in function values**, i.e., sublinear \\(O(1/k)\\)-type (convex) and linear (strongly convex) decrease in expected suboptimality under appropriate oracle-accuracy/probability assumptions.",
        "relevance_category": "PARTIAL_PROGRESS",
        "exact_match": null,
        "connection": "This work directly tackles the core difficulty in the open problem: the step size is random because it is chosen by a stochastic line search, and decisions are made using noisy function/gradient information. The analysis framework (accuracy events holding with fixed probability, expected complexity bounds, and handling backtracking randomness) is a strong candidate to be transplanted into a VS-SQN setting by replacing the steepest-descent direction with a quasi-Newton direction and adding the dependence between the direction noise and the line-search noise. While it does not analyze quasi-Newton directions, it provides one of the cleanest non-asymptotic analyses of stochastic Armijo-type line searches."
      },
      {
        "bibtex": "@article{JalilzadehNedicShanbhagYousefian2018VSSQN,\n  author       = {Jalilzadeh, Afrooz and Nedi{\\'c}, Angelia and Shanbhag, Uday V. and Yousefian, Farzad},\n  title        = {A Variable Sample-size Stochastic Quasi-Newton Method for Smooth and Nonsmooth Stochastic Convex Optimization},\n  journal      = {arXiv preprint},\n  year         = {2018},\n  eprint       = {1804.05368},\n  archivePrefix= {arXiv},\n  primaryClass = {math.OC},\n  doi          = {10.48550/arXiv.1804.05368}\n}",
        "theorem_or_result": "The paper develops a regularized/smoothed variable-sample-size stochastic quasi-Newton framework (including regularized/smoothed L-BFGS-type updates) for stochastic convex optimization.\n\nKey results (as stated in the abstract; exact theorem statements not accessed here):\n- In strongly convex regimes (including state-dependent noise), the proposed VS-SQN scheme achieves a **non-asymptotic linear rate** in expected suboptimality when sample sizes increase, with an accompanying **oracle complexity bound** for obtaining an \\(\\varepsilon\\)-solution.\n- In merely convex smooth regimes, it provides **sublinear rates** (e.g., near \\(O(1/k)\\) up to a tunable exponent), and discusses how to obtain rates when Lipschitz/strong convexity parameters are unavailable by using regularization/smoothing strategies.",
        "relevance_category": "PARTIAL_PROGRESS",
        "exact_match": null,
        "connection": "This is one of the most directly aligned papers to the VS-SQN part of the open problem: it already couples quasi-Newton updates with increasing sample sizes and provides non-asymptotic convergence rates and oracle complexities in convex/strongly convex settings. However, it does not provide a stochastic line-search rule with a full dependence-aware analysis; stepsizes are handled via prescribed rules/regularization rather than a noisy Armijo/Wolfe procedure. Thus it supplies the VS-SQN backbone and rate targets that a stochastic line-search analysis would need to match."
      },
      {
        "bibtex": "@article{BollapragadaByrdNocedal2018AdaptiveSampling,\n  author       = {Bollapragada, Raghu and Byrd, Richard H. and Nocedal, Jorge},\n  title        = {Adaptive Sampling Strategies for Stochastic Optimization},\n  journal      = {SIAM Journal on Optimization},\n  year         = {2018},\n  volume       = {28},\n  number       = {4},\n  pages        = {3312--3343},\n  doi          = {10.1137/17M1154679},\n  eprint       = {1710.11258},\n  archivePrefix= {arXiv},\n  primaryClass = {math.OC}\n}",
        "theorem_or_result": "The paper proposes adaptive sample-size rules for stochastic gradient methods based on an **inner product test** designed to ensure that the stochastic direction is a descent direction with high probability.\n\nKey results (from the abstract; exact theorem statements not accessed here):\n- The inner product test is shown to improve on earlier norm-based tests for controlling sample size.\n- The resulting adaptive-sampling method is **globally convergent for nonconvex functions** and enjoys a **global linear convergence rate on strongly convex functions**, with sample sizes increased only as needed to control stochasticity.",
        "relevance_category": "PARTIAL_PROGRESS",
        "exact_match": null,
        "connection": "This paper supplies a principled, implementable way to choose the batch sizes \\(N_k\\) so that descent properties hold with high probability\u2014exactly the kind of sampling rule needed to make stochastic Armijo/Wolfe tests meaningful. In a VS-SQN line-search method, a close analogue of this inner-product test can be used to control the angle between the quasi-Newton direction based on \\(g_k\\) and the true direction based on \\(\\nabla f(x_k)\\), reducing the probability that the line search rejects good steps due to noise. The open problem\u2019s oracle-complexity accounting would naturally combine such sample-size tests with line-search sample costs."
      },
      {
        "bibtex": "@article{JinScheinbergXie2023SampleComplexityAdaptiveOracles,\n  author       = {Jin, Billy and Scheinberg, Katya and Xie, Miaolan},\n  title        = {Sample Complexity Analysis for Adaptive Optimization Algorithms with Stochastic Oracles},\n  journal      = {arXiv preprint},\n  year         = {2023},\n  eprint       = {2303.06838},\n  archivePrefix= {arXiv},\n  primaryClass = {math.OC},\n  doi          = {10.48550/arXiv.2303.06838}\n}",
        "theorem_or_result": "The paper develops a framework to analyze **total oracle/sample complexity** (not just iteration complexity) of adaptive stochastic methods whose step sizes are updated based on stochastic progress measures.\n\nMain result (from the abstract; exact theorem statements not accessed here): it proves a **high-probability lower bound** on the adaptive step-size parameter for a broad class of methods, and uses it to derive expected/high-probability bounds on **total oracle complexity**. It then applies the framework to obtain total sample-complexity guarantees for two adaptive stochastic methods (including STORM and SASS) in expectation-valued risk minimization.",
        "relevance_category": "PARTIAL_PROGRESS",
        "exact_match": null,
        "connection": "A major obstacle in the open problem is that line-search stepsizes \\(\\alpha_k\\) are random, can become very small due to stochastic rejections, and can inflate the overall oracle complexity because more accurate function/gradient estimates are needed when \\(\\alpha_k\\) shrinks. This paper addresses that exact structural issue by proving lower bounds on adaptive step sizes and converting iteration complexity into oracle complexity in stochastic-oracle settings. While it does not specialize to quasi-Newton directions, its step-size lower-bound and sample-complexity machinery appears directly reusable for stochastic line-search VS-SQN analyses."
      },
      {
        "bibtex": "@article{VaswaniBabanezhad2025ArmijoLSStochasticGD,\n  author       = {Vaswani, Sharan and Babanezhad, Reza},\n  title        = {Armijo Line-search Can Make (Stochastic) Gradient Descent Provably Faster},\n  journal      = {arXiv preprint},\n  year         = {2025},\n  eprint       = {2503.00229},\n  archivePrefix= {arXiv},\n  primaryClass = {math.OC},\n  doi          = {10.48550/arXiv.2503.00229}\n}",
        "theorem_or_result": "The paper strengthens convergence theory for gradient descent with Armijo line search and extends it to a stochastic setting under additional structure.\n\nResults (from the abstract; exact theorem statements not accessed here):\n- For smooth convex objectives satisfying a \u201cnon-uniform smoothness\u201d condition (including certain logistic-regression/multiclass objectives), GD with Armijo line search can achieve faster rates than GD with a fixed \\(1/L\\) step, including (in those examples) linear convergence.\n- Under an interpolation assumption (zero training loss), stochastic GD equipped with a stochastic Armijo-style line search can match the fast convergence of deterministic GD with Armijo line search.",
        "relevance_category": "PARTIAL_PROGRESS",
        "exact_match": null,
        "connection": "This is directly about eliminating prior knowledge of the smoothness constant \\(L\\) via Armijo line-search, including in stochastic settings. Although its strongest stochastic guarantees rely on interpolation (which is not assumed in the open problem), it demonstrates that stochastic Armijo tests can be analyzed non-asymptotically and can remove explicit dependence on unknown \\(L\\) in stepsize selection. Extending its ideas beyond interpolation and from steepest-descent to quasi-Newton directions aligns closely with the open problem."
      },
      {
        "bibtex": "@article{WillsSchon2018StochasticQNAdaptiveSteps,\n  author       = {Wills, Adrian and Sch{\\\"o}n, Thomas B.},\n  title        = {Stochastic quasi-{N}ewton with adaptive step lengths for large-scale problems},\n  journal      = {arXiv preprint},\n  year         = {2018},\n  eprint       = {1802.04310},\n  archivePrefix= {arXiv},\n  primaryClass = {math.OC},\n  doi          = {10.48550/arXiv.1802.04310}\n}",
        "theorem_or_result": "The paper proposes a stochastic quasi-Newton method that incorporates an adaptive step-length mechanism described as a stochastic line search.\n\nResult (from the abstract; exact theorem statements not accessed here): it introduces an auxiliary-variable construction enabling a stochastic line-search-like adaptation of the step length while maintaining an inverse-Hessian approximation based on a receding history of iterates/gradients, and reports numerical evidence on very large-scale problems.",
        "relevance_category": "RELATED_TOOL",
        "exact_match": null,
        "connection": "This source is highly relevant algorithmically: it is explicitly a stochastic quasi-Newton method with adaptive (line-search-like) step-length selection, matching the open problem\u2019s aim of avoiding prior knowledge of \\(L\\). However, the publicly accessible summary does not clearly indicate non-asymptotic rates or an oracle-complexity analysis that handles the dependence between \\(\\alpha_k\\) and direction/noise, so it serves as a related tool and design reference rather than a solution. Its constructions may inspire practical line-search rules for VS-SQN."
      },
      {
        "bibtex": "@article{WillsJidlingSchon2018FastStochasticQN,\n  author       = {Wills, Adrian and Jidling, Carl and Sch{\\\"o}n, Thomas B.},\n  title        = {A fast quasi-{N}ewton-type method for large-scale stochastic optimisation},\n  journal      = {arXiv preprint},\n  year         = {2018},\n  eprint       = {1810.01269},\n  archivePrefix= {arXiv},\n  primaryClass = {math.OC},\n  doi          = {10.48550/arXiv.1810.01269}\n}",
        "theorem_or_result": "The paper proposes a quasi-Newton-type stochastic optimization method combined with a stochastic backtracking procedure based on a Wolfe condition.\n\nResult (from the abstract; exact theorem statements not accessed here): it provides \u201cseveral theoretical results guaranteeing its performance,\u201d alongside an adaptive stochastic line search that backtracks until (a stochastic version of) the Wolfe condition is met.",
        "relevance_category": "RELATED_TOOL",
        "exact_match": null,
        "connection": "This paper is directly in the family of methods sought by the open problem: quasi-Newton-type directions with a stochastic Wolfe/backtracking line search. While details of the non-asymptotic guarantees and how they treat dependence between the step selection and direction noise are not verifiable from the accessible snippet alone, the method\u2019s architecture aligns strongly with VS-SQN line-search design. It is thus a relevant algorithmic and conceptual reference, potentially complementary to more formal complexity frameworks like Paquette\u2013Scheinberg and Jin\u2013Scheinberg\u2013Xie."
      },
      {
        "bibtex": "@article{ByrdHansenNocedalSinger2016SQN,\n  author       = {Byrd, Richard H. and Hansen, Samantha L. and Nocedal, Jorge and Singer, Yoram},\n  title        = {A Stochastic Quasi-{N}ewton Method for Large-Scale Optimization},\n  journal      = {SIAM Journal on Optimization},\n  year         = {2016},\n  volume       = {26},\n  number       = {2},\n  pages        = {1008--1031},\n  doi          = {10.1137/140954362},\n  eprint       = {1401.7020},\n  archivePrefix= {arXiv},\n  primaryClass = {cs.LG}\n}",
        "theorem_or_result": "The paper introduces the SQN method: a stochastic limited-memory BFGS approach where curvature information is gathered at spaced intervals using (subsampled) Hessian\u2013vector products to stabilize quasi-Newton updates under noise.\n\nResult (from the abstract; exact theorem statements not accessed here): it argues (and demonstrates empirically) that using pointwise curvature information obtained at regular intervals is more robust than using noisy gradient differences at every iteration, enabling efficient scalable stochastic quasi-Newton updates for large-scale learning problems.",
        "relevance_category": "BACKGROUND",
        "exact_match": null,
        "connection": "This is a foundational stochastic quasi-Newton reference and motivates many later VS-SQN/L-BFGS variants. It does not focus on stochastic line search or non-asymptotic oracle complexity with random \\(\\alpha_k\\), but it supplies key quasi-Newton stabilization mechanisms (curvature sampling, limited-memory updates) that any line-search VS-SQN analysis would need to incorporate. In particular, ensuring positive definiteness and controlling eigenvalues of \\(H_k\\) is crucial when coupling with line-search acceptance tests."
      },
      {
        "bibtex": "@article{JinJiangMokhtari2024NonasymptoticBFGSArmijoWolfe,\n  author       = {Jin, Qiujiang and Jiang, Ruichen and Mokhtari, Aryan},\n  title        = {Non-asymptotic Global Convergence Analysis of {BFGS} with the Armijo--Wolfe Line Search},\n  journal      = {arXiv preprint},\n  year         = {2024},\n  eprint       = {2404.16731},\n  archivePrefix= {arXiv},\n  primaryClass = {math.OC},\n  doi          = {10.48550/arXiv.2404.16731}\n}",
        "theorem_or_result": "The paper provides explicit global non-asymptotic rates for deterministic BFGS with an inexact line search satisfying Armijo\u2013Wolfe conditions.\n\nMain results (as stated in the abstract):\n- For \\(\\mu\\)-strongly convex, \\(L\\)-smooth objectives, BFGS with Armijo\u2013Wolfe achieves a **global linear rate** of the form \\((1-1/\\kappa)^t\\) with \\(\\kappa=L/\\mu\\).\n- If the Hessian is Lipschitz continuous, it establishes a linear rate depending only on the line-search parameters (independent of \\(\\kappa\\)) and also a global superlinear rate characterized as \\(\\mathcal O((1/t)^t)\\).",
        "relevance_category": "RELATED_TOOL",
        "exact_match": null,
        "connection": "Although deterministic, this is highly relevant because it gives a modern, explicit complexity characterization for BFGS coupled with Armijo\u2013Wolfe line searches\u2014exactly the deterministic backbone one would like to recover \u201cin expectation\u201d in the stochastic setting. In the open problem, one challenge is that noisy line-search decisions break the standard deterministic proof structure; this paper\u2019s explicit rates clarify what stochastic analogues should target. It can serve as the deterministic part of a proof that conditions on good sampling/accuracy events for stochastic function and gradient estimates."
      },
      {
        "bibtex": "@article{MahsereciHennig2017ProbabilisticLineSearch,\n  author       = {Mahsereci, Maren and Hennig, Philipp},\n  title        = {Probabilistic Line Searches for Stochastic Optimization},\n  journal      = {Journal of Machine Learning Research},\n  year         = {2017},\n  volume       = {18},\n  number       = {119},\n  pages        = {1--59},\n  eprint       = {1703.10034},\n  archivePrefix= {arXiv},\n  primaryClass = {stat.ML}\n}",
        "theorem_or_result": "The paper develops a **probabilistic line search** for stochastic optimization by maintaining a Gaussian-process surrogate model of the univariate objective along a search direction and using a probabilistic assessment of Wolfe-type conditions.\n\nResult (as described in the abstract and JMLR entry): the method replaces deterministic accept/reject line-search logic with a probabilistic belief over satisfaction of Wolfe conditions, aiming to remove manual learning-rate tuning; the contribution is primarily algorithmic (no explicit non-asymptotic convergence-rate theorem is claimed in the accessible summary).",
        "relevance_category": "RELATED_TOOL",
        "exact_match": null,
        "connection": "This is a prominent alternative approach to stochastic line search that explicitly models uncertainty in function and directional derivative evaluations, rather than trying to enforce (noisy) Armijo/Wolfe conditions directly. For the open problem, it provides a design paradigm for choosing \\(\\alpha_k\\) that acknowledges stochastic dependence and uncertainty, potentially simplifying analysis by working with probabilistic acceptance events. However, it does not directly deliver the kind of rate/oracle-complexity guarantees requested for convex VS-SQN, so it is best viewed as an algorithmic tool and inspiration."
      },
      {
        "bibtex": "@article{LucchiMcWilliamsHofmann2015VITE,\n  author       = {Lucchi, Aur{\\'e}lien and McWilliams, Brian and Hofmann, Thomas},\n  title        = {A Variance Reduced Stochastic {N}ewton Method},\n  journal      = {arXiv preprint},\n  year         = {2015},\n  eprint       = {1503.08316},\n  archivePrefix= {arXiv},\n  primaryClass = {math.OC},\n  doi          = {10.48550/arXiv.1503.08316}\n}",
        "theorem_or_result": "The paper proposes VITE, a stochastic quasi-Newton/Newton-type method that combines curvature information with variance reduction.\n\nKey result (from the abstract; exact theorem statements not accessed here): for smooth strongly convex objectives, VITE achieves a **geometric (linear) convergence rate** to the optimum with a **constant stepsize**, leveraging variance-reduced gradient estimators while allowing approximate Hessian information.",
        "relevance_category": "RELATED_TOOL",
        "exact_match": null,
        "connection": "While not a line-search method, this work is important because it shows how stochastic quasi-Newton directions can recover linear rates non-asymptotically when gradient variance is controlled (via variance reduction). In the open problem, one can view a variable sample-size strategy as another way to reduce variance; VITE provides proof patterns for combining curvature approximations with variance control. Integrating such variance-reduction ideas with a stochastic line search could help decouple \\(\\alpha_k\\) from the direction noise, making dependence issues more tractable."
      }
    ]
  },
  "costs": {
    "extractor_total_cost_usd": 0.11015725,
    "verifier_total_cost_usd": 0.29482775,
    "literature_total_cost_usd": 0.4842145,
    "total_cost_usd": 0.8891994999999999
  },
  "provenance": {
    "problem_source": "data/batch10_high/problems/W2797791799/paper/problems.json",
    "verification_source": "data/batch10_high/verifications/W2797791799_p0_verification.json",
    "literature_source": "data/batch10_high/literature_reviews/W2797791799_p0_literature_review.json"
  }
}